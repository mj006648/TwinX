# FILE: argocd/twinx/apps/job-schedules/02-batch-merge-schedule.yaml
# 신규 파일: 주기적으로 dev 브랜치의 데이터를 main으로 병합하는 배치 잡을 스케줄링

apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: ScheduledSparkApplication
metadata:
  name: periodic-merge-to-main
  namespace: spark-operator
spec:
  # 10분마다 실행 (Cron 표현식)
  schedule: "*/10 * * * *"
  concurrencyPolicy: "Forbid" # 이전 잡이 실행 중이면 다음 잡은 실행하지 않음
  template:
    type: Python
    mode: cluster
    image: "ich6648/spark-iceberg-nessie-kafka:9.0"
    imagePullPolicy: Always
    # [핵심] 발행(merge) 스크립트를 지정
    mainApplicationFile: "local:///mnt/scripts/periodic_merge_to_main.py"
    sparkVersion: "3.5.0"
    restartPolicy:
      type: OnFailure # 배치 잡은 실패 시에만 재시도
    driver:
      serviceAccount: spark-sa
      cores: 1
      memory: "1g"
      envFrom:
        - secretRef:
            name: s3-creds-for-streaming-spark
      volumeMounts:
        - name: scripts-volume
          mountPath: /mnt/scripts
    executor:
      instances: 2
      cores: 1
      memory: "1g"
      envFrom:
        - secretRef:
            name: s3-creds-for-streaming-spark
      volumeMounts:
        - name: scripts-volume
          mountPath: /mnt/scripts
    volumes:
      - name: scripts-volume
        configMap:
          name: spark-scripts

